### 论文学习

随着生物医学文献的迅速增长，生物医学文本挖掘变得越来越重要。随着机器学习技术的发展，从生物医学文献中提取有价值的信息在研究者中得到了广泛的应用，并且深度学习促进了有效的生物医学文本挖掘模型的发展。然而，由于深度学习模式需要大量的训练数据，由于缺乏生物医学领域的训练数据，将深度学习应用于生物医学文本挖掘往往是不成功的。最近关于文本语料库语境化语言表示模型的研究揭示了利用大量无注释的生物医学文本语料库的可能性。

他们介绍了BioBERT（生物医学文本挖掘中转换器的双向编码器表示），是一个大型生物医学语料库的领域特定语言表示模型。基于BERT体系结构，BioBERT有效地将知识从大量的生物医学文本转移到生物医学文本挖掘模型，并对其进行了最小任务特定结构的修改。 BERT表现出了与以前最先进模型媲美的表演，而在以下三项具有代表性的生物医学文本挖掘任务上BioBERT则显著更优：生物医学命名实体识别（0.51%提升）、生物医学关系提取（3.49%提升）和生物医学问题回答（9.61%提升）。

他们做了预训练权重的BioBERT<https://github.com/naver/biobert-pretrained>，

并且对BioBert进行了微调<https://github.com/dmis-lab/biobert>。

#### 引言

机器学习使生物医学文本挖掘模型的发展成为可能。而深度学习模型是高效的，因为它们需要更少的特征工程。近几年来，长时记忆(LSTM)和条件随机场(CRF)在生物医学命名实体识别(NER)方面的性能有了很大的提高。其他基于深度学习的模型也改进了生物医学文本挖掘任务，比如如关系提取(Bhasuran和Natarajan，2018)和问题回答（Wiese *et al.*, 2017）。深度学习需要大量的培训数据，然而，在生物医学文本挖掘任务中，构建一个大型培训集非常昂贵，因为它需要在生物医学领域使用专家。

在本文中，我们使用转移学习来解决缺乏培训数据的问题。Word2vec(Mikolov等人，2013年)，从未加注释的文本中提取知识，是自然语言处理的主要进展之一。然而，在生物医学文本挖掘任务中，由于生物医学语料库与通用领域语料库上的巨大差异，在应用于生物医学数据时，word 2vec需要修改。ELMo（Peters等人，2018)和BERT(Devlin等人，2018)的最新发展证明了语境化表示的有效性。

而从一般领域语料库(如维基百科)中提取的上下文表示则被用于生物医学文本挖掘研究。很少有研究聚焦于直接从生物医学语料库中提取语境表征。

本文介绍了一种基于BERT的上下文化语言表示模型BioBERT，用于生物医学文本挖掘任务。我们对一般和生物医学领域语料库的不同组合进行了预处理，以了解其在特定领域语料库中的生物医学文本挖掘任务的性能变化。

#### 途径

本文提出了一种用于生物医学领域的预训练语言表示模型BioBERT。首先，我们使用在一般领域语料库上预先训练的BERT初始化了BioBERT。然后，对BioBERT进行生物医学领域语料库的预培训。Biobert在几个数据集帮助下进行了微调，以显示其在每个生物医学文本挖掘任务中的有效性。

和BERT类似，BioBERT需要最少数量的特定任务的参数。此外，我们还对普通领域语料库和生物医学语料库的不同组合和大小进行了实验，以分析每个语料库在预训练中的作用和作用。

#### 方法

这部分简要地讨论了最近提出的BERT，然后详细描述了BioBERT的预训练过程。此外还解释了针对不同任务的BioBERT的微调，在这些任务中，需要最少的领域特定工程。

**BERT**

从大量未加注释的文本中学习单词表示是一种由来已久的方法。BERT（Devlin等人，2018）是一种上下文化的词表示模型，它基于使用双向变压器的语言模型进行预训练。

由于语言建模的本质是无法看到未来的单词，以前的双向语言模型(biLM)仅限于两个单向语言模型的组合。然而，BERT使用了一种masked语言模型，该模型可以预测序列中随机隐藏的单词，从而可以用于学习更好的双向表示。结合下一个句子预测任务和较大的文本语料库，BERT在大多数nlp任务上获得最先进的性能，并且有最小的任务特定架构修改。

**BioBERT预训练	**

语言表示模型的性能在很大程度上取决于语料库的大小和质量，而这些语料库是经过预先训练的。由于BERT被设计为通用语言表示模型，其接受了英语维基百科和图书语料库的预培训。然而，生物医学领域的文本包含了相当多的内容。因此，为理解通用语言而设计的nlp模型在生物医学文本挖掘任务中往往表现不佳。即使是生物医学领域的人类专家在生物医学文本挖掘任务中也表现出很低的性能，这表明这些任务是非常困难的。

为了将文本挖掘模型应用于生物医学文本，一些研究人员对生物医学公司领域特定语言表示模型进行了培训。培训过的PubMed摘要和PubMed中心全文的Word2vec对生物医学文本挖掘模型有实质性的影响。和以前的工作一样，我们也在PubMed摘要(PubMed)和PubMed中央全文文章(PMC)上对我们的模型BioBERT进行了预培训。

我们的训练过程程序旨在回答以下三个研究问题：首先，BERT在进行生物医学领域语料库和仅通用领域语料库的预培训时，在生物医学文本挖掘任务中的性能有何不同？第二，预训语料库的最佳组合是什么，从而在生物医学文本挖掘任务中获得最佳结果。第三，领域特定的语料库应该有多大才能有效地训练BioBERT？为了回答这些问题，我们对BERT进行不同文本语料库组合的培训，同时改变语料库的大小。

**微调BioBERT**

与BERT一样，BioBERT被应用于各种下游文本挖掘任务，同时只需要最小的架构修改。微调Bert(BioBERT)时常见的设置之一是使用WordPiese标记化，其能够缓解词汇外的问题。使用WordPiess标记化，任何新词都可以用一些常见的子词来表示。

我们简要描述了三个具有特定任务设置的生物医学文本挖掘任务：

**命名实体识别**是最基本的生物医学文本挖掘任务之一，它涉及到识别生物医学语料库中许多特定领域的专有名词。当对BioBERT(或BERT)进行微调时，我们发现无论选择BIO还是BIOES，模型的性能都是相同的，而以前基于LSTM+CRF的模型在BIOES中表现得更好。

**关系提取**是生物医学语料库中命名实体关系的分类任务。由于关系提取可以被视为一个句子分类任务，我们使用了BERT的原始版本的句子分类器，它使用[CLS]令牌进行分类。在关系提取中有两种表示目标命名实体的方法，我们可以使用预定义的标记匿名命名实体，我们也可以用预定义的标记包装目标实体。我们发现第一种方法一致地取得了较好的结果。

**问题回答**是一项用自然语言回答相关段落的问题的任务。对于QA的BioBERT微调，我们使用了为 SQuAD设计的BERT模型。作为用于生物医学文本挖掘任务的现有QA数据集并不总是假设提取设置。我们将无法回答的问题排除在训练集之外，但认为它们在测试集中是错误的预测。此外，我们使用了维泽等人(2017)的训练前程序，其中使用了SQuAD。并且它不断地提高了BioBERT在BioASQ数据集上的性能。